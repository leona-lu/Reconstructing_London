negative = sum(sentiment == "negative")) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = test_df$texts[[1]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing"), by = "word") %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens %>%
inner_join(get_sentiments("bing")) %>%
# dplyr:: count(sentiment) %>%
# spread(sentiment, n, fill = 0) %>%
dplyr:: group_by(sentiment) %>%
dplyr:: summarise(
positive = sum(sentiment =="positive"),
negative = sum(sentiment == "negative"))
tokens <- data_frame(text = test_df$texts[[1]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing"), by = "word") %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens %>%
inner_join(get_sentiments("bing")) %>%
# dplyr:: count(sentiment) %>%
# spread(sentiment, n, fill = 0) %>%
dplyr:: group_by(sentiment) %>%
dplyr:: mutate(
positive = sum(sentiment =="positive"),
negative = sum(sentiment == "negative"))
tokens <- data_frame(text = test_df$texts[[1]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing"), by = "word") %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens %>%
inner_join(get_sentiments("bing")) %>%
# dplyr:: count(sentiment) %>%
# spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(
positive = sum(sentiment =="positive"),
negative = sum(sentiment == "negative"))
.get_sentiment <- function(dataframe){
tokens <- data_frame(text = dataframe$texts[[1]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
result <- tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
for (i in 2:nrow(dataframe)){
tokens <- data_frame(text = dataframe$texts[[i]]) %>%
unnest_tokens(word, text)
result_updated <- tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
result <- rbind(result, result_updated)
}
result$Date <- dataframe$Date
result$TCP <- dataframe$TCP
return(result)
}
Case_results <- tcpSearch(term <- "St. Paul's", field = "Terms")
# tcpDownload(Case_results)
.importTexts <- function(dataframe, normalize = TRUE) {
dataframe$path <- paste0("/Users/yuchenlu/Desktop/Test/Case_result/",dataframe$TCP,".xml")
dataframe$texts <- lapply(dataframe$path,.cleanup)
return(dataframe)
}
Case_results <-.importTexts(Case_results)
Case_Sentiment_result <- .get_sentiment(Case_results)
Hapax_Result <- Hapax_richness(Case_results)
TTR_Result <- Lexical_richness(Case_results)
total_Lexical_Richness <- merge(TTR_Result,Hapax_Result,by="TCP")
df <- merge(total_Lexical_Richness[c("TCP","TTR","Hapax")],
Case_Sentiment_result[c("sentiment","Date","TCP")], by = "TCP")
.importTexts <- function(dataframe, normalize = TRUE) {
dataframe$path <- paste0("/Users/yuchenlu/Desktop/Test/Total_Case/",dataframe$TCP,".xml")
dataframe$texts <- lapply(dataframe$path,.cleanup)
return(dataframe)
}
total_result <-.importTexts(total_result)
Total_Sentiment_result <- .get_sentiment(total_result)
Total_Sentiment_result <- Total_Sentiment_result[-c(21)]
Total_Sentiment_result <- Total_Sentiment_result[-(21)]
Total_Sentiment_result <- Total_Sentiment_result[-(21)]
Total_Sentiment_result <- Total_Sentiment_result[-c(21),]
Total_Sentiment_result <- .get_sentiment(total_result)
tokens <- data_frame(text = dataframe$texts[[21]]) %>%
unnest_tokens(word, text)
tokens <- data_frame(text = total_result$texts[[21]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
result <- tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[21]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[22]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
Title_result <- tcpSearch(term <- "St. Paul's", field = "Title")
Term_results <- tcpSearch(term <- "St. Paul's", field = "Terms")
# tcpDownload(Case_results)
total_result <- rbind(Title_result,Term_results)
total_result <- total_result[!duplicated(total_result$TCP),]
# tcpDownload(total_result)
.importTexts <- function(dataframe, normalize = TRUE) {
dataframe$path <- paste0("/Users/yuchenlu/Desktop/Test/Total_Case/",dataframe$TCP,".xml")
dataframe$texts <- lapply(dataframe$path,.cleanup)
return(dataframe)
}
total_result <-.importTexts(total_result)
Total_Sentiment_result <- Total_Sentiment_result[-c(22),]
Total_Sentiment_result <- .get_sentiment(total_result)
tokens <- data_frame(text = total_result$texts[[23]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[24]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[25]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[26]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[27]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[28]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[29]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[30]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[31]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[32]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[33]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[34]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[35]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[36]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[37]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[38]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[39]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[40]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[41]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[42]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[43]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[44]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[45]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[46]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[47]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[48]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[49]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[50]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[51]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[52]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[53]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[54]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[55]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
tokens <- data_frame(text = total_result$texts[[56]]) %>%
unnest_tokens(word, text)
tokens <- data_frame(text = total_result$texts[[22]]) %>%
unnest_tokens(word, text)
# lexicon = c("bing", "afinn", "loughran", "nrc")
tokens %>%
inner_join(get_sentiments("bing")) %>%
dplyr:: count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
dplyr:: mutate(sentiment = positive - negative)
Title_result <- tcpSearch(term <- "St. Paul's", field = "Title")
Title_result <- tcpSearch(term <- "St. Paul's", field = "Title")
Term_results <- tcpSearch(term <- "St. Paul's", field = "Terms")
# tcpDownload(Case_results)
total_result <- rbind(Title_result,Term_results)
total_result <- total_result[!duplicated(total_result$TCP),]
# tcpDownload(total_result)
Total_Sentiment_result <- Total_Sentiment_result[-c(22),]
# Total_Sentiment_result <- .get_sentiment(total_result)
Total_Sentiment_result <- Total_Sentiment_result[-c(22),]
# Total_Sentiment_result <- .get_sentiment(total_result)
View(total_result)
Total_Sentiment_result[-c(22),]
# Total_Sentiment_result <- .get_sentiment(total_result)
total_result[-c(22),]
# Total_Sentiment_result <- .get_sentiment(total_result)
total_result <- total_result[-c(22),]
# Total_Sentiment_result <- .get_sentiment(total_result)
total_result <- total_result[-c(22),]
Total_Sentiment_result <- .get_sentiment(total_result)
Title_result <- tcpSearch(term <- "St. Paul's", field = "Title")
Term_results <- tcpSearch(term <- "St. Paul's", field = "Terms")
# tcpDownload(Case_results)
total_result <- rbind(Title_result,Term_results)
total_result <- total_result[!duplicated(total_result$TCP),]
# tcpDownload(total_result)
total_result <- total_result[-c(22),]
Total_Sentiment_result <- .get_sentiment(total_result)
# total_result <- total_result[-c(22),]
Total_Sentiment_result <- .get_sentiment(total_result)
# total_result <- total_result[-c(22),]
Total_Sentiment_result <- .get_sentiment(total_result)
# total_result <- total_result[-c(22),]
Total_Sentiment_result <- .get_sentiment(total_result)
# total_result <- total_result[-c(22),]
Total_Sentiment_result <- .get_sentiment(total_result)
# total_result <- total_result[-c(22),]
Total_Sentiment_result <- .get_sentiment(total_result)
.importTexts <- function(dataframe, normalize = TRUE) {
dataframe$path <- paste0("/Users/yuchenlu/Desktop/Test/Total_Case/",dataframe$TCP,".xml")
dataframe$texts <- lapply(dataframe$path,.cleanup)
return(dataframe)
}
total_result <-.importTexts(total_result)
# total_result <- total_result[-c(22),]
Total_Sentiment_result <- .get_sentiment(total_result)
View(total_Lexical_Richness)
View(Total_Sentiment_result)
Hapax_Result <- Hapax_richness(total_result)
TTR_Result <- Lexical_richness(total_result)
total_Lexical_Richness <- merge(TTR_Result,Hapax_Result,by="TCP")
df <- merge(total_Lexical_Richness[c("TCP","TTR","Hapax")],
Case_Sentiment_result[c("sentiment","Date","TCP")], by = "TCP")
df <- merge(total_Lexical_Richness[c("TCP","TTR","Hapax")],
Total_Sentiment_result[c("sentiment","Date","TCP")], by = "TCP")
View(df)
View(df)
knitr::opts_chunk$set(echo = TRUE)
library(text2vec)
sample <- total_result$texts[[1]]
# Create wordlist
sample <- total_result$texts[[1]]
tokens <- data_frame(text = sample) %>%
unnest_tokens(word, text)
# Create wordlist
View(tokens)
sample <- total_result$texts[[1]]
tokens <- list(text = sample) %>%
unnest_tokens(word, text)
sample <- total_result$texts[[1]]
tokens <- data_frame(text = sample) %>%
unnest_tokens(word, text) %>%
as.list()
# Create wordlist
# Itokenize word-list
it <- itoken(shakes_words_ls, progressbar = FALSE)
# Load Sample Text
sample <- total_result$texts[[1]]
# Create word-list
word_list <- data_frame(text = sample) %>%
unnest_tokens(word, text) %>%
as.list()
# Itokenize word-list
it <- itoken(word_list, progressbar = FALSE)
# Itokenize word-list
it <- itoken(word_list, progressbar = FALSE)
shakes_vocab = create_vocabulary(it)
shakes_vocab = prune_vocabulary(shakes_vocab, term_count_min = 5)
shakes_vocab
# maps words to indices
vectorizer <- vocab_vectorizer(vocab)
# Itokenize word-list
it <- itoken(word_list, progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(shakes_vocab, term_count_min = 5)
# maps words to indices
vectorizer <- vocab_vectorizer(vocab)
# use window of 10 for context words
tcm  <- create_tcm(it, vectorizer, skip_grams_window = 10)
View(tcm)
