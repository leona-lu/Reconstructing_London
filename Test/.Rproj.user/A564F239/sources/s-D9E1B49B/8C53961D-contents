# Clean-Up Function
.cleanup <- function(filepath, stopwords = c(), normalize = TRUE) {
  stopwords <- scan("/Users/yuchenlu/Desktop/Reconstructing_London/stopwords.txt", what="", sep="\n")
  
  if (length(grep(".txt",filepath)) == 1) {
    text = scan(filepath,what="character",sep="\n", fileEncoding = "UTF-8")
    text = paste(text, collapse= " ")
    
  } else if (length(grep(".xml",filepath)) == 1) {
    parsedText = htmlTreeParse(filepath,useInternalNodes = TRUE)
    nodes = getNodeSet(parsedText,"//text")
    text = lapply(nodes,xmlValue)
  }
  
  text = gsub("non-Latin alphabet", " ", text)
  text = gsub("1 page duplicate", " ", text)
  
  if (normalize == TRUE) {
    text = gsub("ſ", "s", text)
    #text = gsub("?.¿", "s", text)
    text = gsub("[0-9]", "", text)
    text = gsub("vv", "w", text)
    text = gsub("'d ", "ed ", text)
    text = gsub("'ring ", "ering ", text)
  }
  
  text = strsplit(text,"\\W")
  text = unlist(text)
  text = text[text!=""]
  
  if (normalize == TRUE) {
    text = tolower(text)
    text = text[text %in% stopwords == FALSE]
    if (any(grep("[^\x20-\x7E]",text))) text = text[-grep("[^\x20-\x7E]",text)]
  }
  
  text = paste(text, collapse = " ")
  return(text)
}


# Import text into CSV
.importTexts <- function(dataframe, normalize = TRUE) {
  dataframe$path <- paste0("/Users/yuchenlu/Desktop/Reconstructing_London/Data",dataframe$TCP,".xml")
  dataframe$texts <- lapply(dataframe$path,.cleanup)
  return(dataframe)
}

# Read Overall Index DF
df <- read.csv("/Users/yuchenlu/Desktop/Reconstructing_London/index.csv")
# Separate DF into 10 CSV 
test_df <- df[c(2201,4402,6603),]
test_df <- .importTexts(test_df)
test_df$texts <- vapply(test_df$texts, paste, collapse = ", ", character(1L))
write.csv(test_df,'Cleaned_Data_test.csv')
